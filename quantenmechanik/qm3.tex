\documentclass[a4paper,german,12pt,smallheadings]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{braket}
\usepackage{graphicx}
\usepackage{babel}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{enumerate}
%\usepackage{wrapfig}
\usepackage[thinspace,thinqspace,squaren,textstyle]{SIunits}
\restylefloat{table}
\geometry{a4paper, top=15mm, left=20mm, right=40mm, bottom=20mm, headsep=10mm, footskip=12mm}
\linespread{1.5}
\setlength\parindent{0pt}
\begin{document}
\begin{center}
\bfseries % Fettdruck einschalten
\sffamily % Serifenlose Schrift
\vspace{-40pt}
Quantum Mechanics, winter semester 2013/2014, exercise sheet 3

Luis Herrmann, Tutor: Adam Nagy
\vspace{-10pt}
\end{center}

\section*{Exercise 1}

\begin{enumerate}[a)]
\item Consider the wave package:
\begin{equation*}
\Phi(x,t)=2C_0\frac{\sin(\frac{\sigma_k}{2})}{u}e^{i(k_0x-w_0t)}\quad \quad u=v_gt-x
\end{equation*}

The corresponding wave function is:
\begin{equation*}
\Phi(x,t)=2C_0\frac{\sin(\frac{u\sigma_k}{2})}{u}
\end{equation*}

We are looking for the Fourier Transformation:
\begin{equation*}
C(k,t)=\int_{-\infty}^{\infty}\Phi(x,t)e^{-iku}du=2C_0\int_{-\infty}^{\infty}\frac{\sin(\frac{u\sigma_k}{2})}{u}e^{-iku}du=2C_0\int_{-\infty}^{\infty}f(u)e^{-iku}du
\end{equation*}

Instead of directly calculating the Fourier Transformation of $f(u)$, it's worth considering the option of checking whether $f(u)$ is the Inverse Transformation of a given function.\\ Generally, if $x(t)$ is a function and $\mathcal{F}_x(s)$ its Fourier Transformation, the Inverse Transformation is given by the formula:
\begin{equation*}
x(t)=\frac{1}{2\pi}\int_{-\infty}^{\infty}\mathcal{F}_x(s)e^{ist}ds
\end{equation*}
In order for a Fourier Transformation to be invertible, the Fourier Transformation has to be an element of Schwartz Space:
\begin{equation*}
S(\mathbb{R})=\{x\in C^{\infty} | t^p x^{(k)} \in L(\mathbb{R}) \quad \forall p,k\in \mathbb{N}_0\}
\end{equation*}

Which is the space of all infinitely often, steadily differentiable and Lebesqe-integrable functions.\\
If $\mathcal{F}_x$ is the Fourier Transformation of $x$, we have the condition: 
\begin{equation*}
\mathcal{F}_x\in S(\mathbb{R}) \quad \Leftrightarrow \quad x\in S(\mathbb{R})
\end{equation*}

This means that, in order to prove the invertability of a Fourier Transformation, it is sufficient to show that either the latter or its Inverse Transformation are elements of Schwartz Space.\\

We assume the rectangle function to be the Fourier Transformation of $f(u)$:
\begin{equation*}
g(k)=\begin{cases}C_0, \quad &|k|\le \alpha\\ 0, \quad &\text{else}\end{cases}
\end{equation*}

Before inverting the transformation, we check whether $f(u),g(k)\in S(\mathbb{R})$. Doing this just for $g(k)$ is sufficient:
\begin{align*}
|k|\le 1: \quad g'(k)=\frac{d}{dk}(1)=0\\
Else: \quad g'(k)=\frac{d}{dk}(0)=0\\
\Rightarrow \quad g'(k)=0
\end{align*}

We obtain the zero polynom, which is infinitely often, steadily differentiable (returning 0 as derivation every time).\\
Obviously, the rectangle function is also infinetely often Riemann- and thus Lebesge-integrable, as $C_0$ and $0$ are finite-dimensional polynoms and, as such, can be integrated infintely often.\\

This means we can calculate the Inverse Transformation of $g(k)$:
\begin{align*}
f(u)& =\frac{1}{2\pi}\int_{-\infty}^{\infty}g(k)e^{iku}dk\\
&=\frac{1}{2\pi}\left[\lim\limits_{b\to \alpha} \int_{-\infty}^{b}0 \; e^{iku}dk + \int_{\alpha}^{-\alpha}1\; e^{iku}dk +\lim\limits_{b \to -\alpha}\int_{b}^{\infty}0 \; e^{iku}dk\right]\\
&=\frac{1}{2\pi}\left[\lim\limits_{b\to \alpha} \underbrace{\int_{-\infty}^{b}0dk}_{=0} + \int_{\alpha}^{-\alpha}e^{iku}dk +\lim\limits_{b \to -\alpha}\underbrace{\int_{b}^{\infty}0dk}_{=0}\right]\\
&=\frac{1}{2\pi}\left[\frac{C_0e^{iku}}{iu}\right]^{\alpha}_{-\alpha}=\frac{C_0}{2\pi}\left[\frac{\cos(\alpha u)+i\sin(\alpha u)-(\cos (\alpha u)-\sin(\alpha u))}{iu}\right]\\
&=C_0\frac{\sin(\alpha u)}{\pi u}
\end{align*}

We were looking for:
\begin{equation*}
C(k,t)=\int_{-\infty}^{\infty}\Phi(x,t)e^{-iku}du
\end{equation*}

Therefore, we set:
\begin{align*}
&\Phi(x,t)=\int_{-\infty}^{\infty}C(k,t)e^{iku}dk\\
\Leftrightarrow \quad &\Phi(x,t)=\nu\int_{-\infty}^{\infty}g(k)e^{iku}dk\\
\Leftrightarrow \quad & 2C_0\frac{sin(u\sigma_k)}{u}=\nu C_0\frac{\sin(\alpha u)}{\pi u}
\end{align*}

In order for our transformation to add up we require that:
\begin{equation*}
\nu=2\pi \quad \quad \alpha=\frac{\sigma_k}{2}
\end{equation*}

And thus obtain:
\begin{equation*}
C(k,t)=\begin{cases}2\pi C_0, \quad &|k|\le\frac{\sigma_k}{2}\\ 0 \quad &\text{else}
\end{cases}
\end{equation*}

\item The norming condition is:
\begin{equation*}
\int_{-\infty}^{\infty}|\Phi(x,t)|^2dx=1
\end{equation*}

Using the parametrisation:
\begin{align*}
&x=v_gt-u\\
&\frac{dx}{du}=-1 \quad \Leftrightarrow \quad dx=-du
\end{align*}
Obviously, $x\to\infty \equiv u\to-\infty$. Thus, our norming condition transforms into:
\begin{align*}
-&\int_{\infty}^{-\infty}|\Phi(u,t)|^2du=1\\
&\int_{-\infty}^{\infty}\frac{4C_0^2\sin^2(\frac{u\sigma_k}{2})}{u^2}du=1\\
 4C_0^2&\underbrace{\int_{-\infty}^{\infty}\frac{\sin^2(\frac{\sigma_k u}{2})}{u^2}}_{=\frac{\pi\sigma_k}{2}}=1\\
\end{align*}

Conveniently, this integral can be solved with Fourier Analysis. Plancherel formula states:
\begin{equation*}
\braket{\mathcal{F}_x,\mathcal{F}_y}_{\mathcal{L}^2}=2\pi\braket{x,y}_{\mathcal{L}^2} \quad \Rightarrow \quad \braket{\mathcal{F}_x,\mathcal{F}_x}_{\mathcal{L}^2}=2\pi\braket{x,x}_{\mathcal{L}^2}
\end{equation*}

Where $\braket{x,x}_{\mathcal{L}^2}$ is the $\mathcal{L}^2$ scalar product induced by the corresponding norm:
\begin{equation*}
||x||_{\mathcal{L}^2}=\left(\int|f(x)|^2dx\right)^{\frac{1}{2}}
\end{equation*}

Using this and the knowledge that the Fourier Transformation of $2\sin^2(\alpha u)/u$ is $rect_{\alpha}(k)$ we show:
\begin{align*}
&\int_{-\infty}^{\infty}\left(\frac{2\sin(\frac{u\sigma_k}{2}}{u}\right)^2=2\pi\int_{-\infty}^{\infty}rect_{\sigma_k/2}(k)dk\\
&\int_{-\infty}^{\infty}\frac{\sin^2(\frac{u\sigma_k}{2})}{u^2}=\frac{\pi}{2}\int_{-sigma_k}^{\sigma_k}rect_{\sigma_k/2}(k)dk\\
&\int_{-\infty}^{\infty}\frac{\sin^2(\frac{u\sigma_k}{2})}{u^2}=\frac{\pi\sigma_k}{2}
\end{align*}

We thus get:
\begin{align*}
& 2C_0^2\pi\sigma_k=1\\
\Leftrightarrow \quad & C_0=\sqrt{\frac{1}{2\sigma_k\pi}}
\end{align*}

\item 
As we found out the rectangle function $C(k,t)$ to be the Fourier transformation of $\Phi(x,t)$, we required the impulse space to cover the interval:
\begin{equation*}
-\frac{\sigma_k}{2}\le k\le\frac{\sigma_k}{2}
\end{equation*}
\begin{center}
...leading up to:\\
$\Delta k=\sigma_k$
\end{center}

Next, we determine $\Delta x$. In order to do this, we consider the probability distribution, given by:
\begin{equation*}
\int_{-x}^{x}|\Phi(x,t)|^2dx=p_{\%}
\end{equation*}

... at a given time, for example $t=0$. It seems to be reasonable to choose:
\begin{align*}
-\pi\le x \le \pi\\
\Delta x =2\pi
\end{align*}
...since this will cover over 90\% of the wave package's residence probability. Unifying the equations for $\Delta k$ and $\Delta x$ and multiplying with the reduced Planck constant, we finally get:
\begin{align*}
&\Delta k \Delta x=2\pi\sigma_k\\
\Leftrightarrow \quad &\Delta k \hbar \Delta x=2\pi\hbar\sigma_k\\
\Leftrightarrow \quad &\Delta p \Delta x=h\sigma_k
\end{align*}

\item For continuous distributions, the impulse and location space are complementary.
This means that, the more we reduce the uncertainty of the impulse associated to $\Delta k$, the bigger the uncertainty about the location of the wave package becomes. Inversely, the more we reduce the incertainty about the location of the wave package, the more uncertain its associated impulse becomes.

\end{enumerate}

\section*{Exercise 2}

\begin{enumerate}[(a)]

\item
\begin{align*}
\bra{\alpha}&=\overline{(7i)}\ket{1}+\overline{(-1)}\ket{2}+\overline{(3)}\ket{3}\\
&=-7i\ket{1}-1\ket{2}+3\ket{3}\\
&=(-7i,-1,3)\\
\bra{\beta}&=\overline{(4i)}\ket{1}+\overline{(2)}\ket{2}\\
&=-4i\ket{1}+2\ket{2}\\
&=(-4i,2,0)
\end{align*}

\item The definiton of the scalar product is:
\begin{equation*}
\braket{\alpha|\beta}=\overline{\braket{\beta|\alpha}}=\sum_{j}^{n}\overline{\alpha_j}\beta_j
\end{equation*}

For our vectors, we obtain the following solutions:
\begin{align*}
\braket{\alpha|\beta}&=\overline{(7i)}(4i) +\overline{(-1)}(2) + \overline{(3)}(0)\\
&=(-7i)(4i)+(-1)(2)=26\\
\braket{\beta|\alpha}&=\overline{(4i)}(7i)+\overline{(2)}(-1)+\overline{3}(0)\\
&=(-4i)(7i)+(2)(-1)=26
\end{align*}

In order for $\ket{\alpha}$ to be normed, it must satisfy the condition $\braket{\alpha|\alpha}=1$:
\begin{align*}
\braket{\alpha|\alpha}&=\overline{(7i)}(7i)+\overline{(-1)}(-1)+\overline{(3)}(3)\\
&=(-7i)(7i)+(-1)^2+(3)^2=59\\
&\Rightarrow \braket{\alpha|\alpha}_n=\frac{7i}{59}\ket{1}-\frac{1}{59}\ket{2}+\frac{3}{59}\ket{3}
\end{align*}
Same for $\ket{\beta}$:
\begin{align*}
\braket{\beta|\beta}&=\overline{(4i)}(4i)+\overline{(2)}(2)\\
&=(-4i)(4i)+(2)^2=18\\
&\Rightarrow \braket{\beta|\beta}_n=\frac{4i}{18}\ket{1}+\frac{2}{18}\ket{2}
\end{align*}

\item The matrix of $\ket{\alpha}\bra{\beta}$ is the outer scalar product. It is a dyadic vector multiplication delivering the matrix:
\begin{equation*}
\ket{\alpha}\bra{\beta}=\begin{pmatrix}
7i\\ -1 \\ 3
\end{pmatrix}\overline{\begin{pmatrix}4i & -1 & 0
\end{pmatrix}}=\begin{pmatrix}
28 & 14i & 0\\ 4i & -2 & 0\\ -12i & 6 & 0
\end{pmatrix}
\end{equation*}
The matrix is not symmetric and thus not hermitian. Furthermore, it has no eigenvalues, since the characteristical polynom is the zero polynom.

\end{enumerate}

\section*{Exercise 3}

\begin{enumerate}
\item Linear:
\begin{align*}
\braket{x,\alpha y+z}&=\sum_{k=1}^{d}\overline{x_k}(\alpha y +z)_k=\sum_{k=1}^{d}\overline{x_k}(\alpha y_k+z_k)=\sum_{k=1}^{d}(\overline{x_k}\alpha y_k + \overline{x_k}z_k)\\&=\alpha\sum_{k=1}^{d}\overline{x_k}y_k+\sum_{k=1}^{d}\overline{x_k}z_k\\
&=\alpha\braket{x,y}+\braket{x,z}
\end{align*}

\item Hermitian:
\begin{align*}
\overline{\braket{y,x}}&=\overline{\left(\sum_{k=1}^{d}\overline{y_k}x_k\right)}=\sum_{k=1}^{d}\overline{\left(\overline{y_k}x_k\right)}=\sum_{k=1}^{d}y_k\overline{x_k}\\
&=\sum_{k=1}^{d}\overline{x_k}y_k\\
&=\braket{x,y}
\end{align*}

\item Positive-definite:
\begin{equation*}
\braket{x,x}=\sum_{k=1}^{d}\overline{x_k}x_k
\end{equation*}
Let $x_n=a_n+b_ni$:
\begin{align*}
\braket{x,x}&=\sum_{k=1}^{d}\overline{(a_k+b_ki)}(a_k+b_ki)=\sum_{k=1}^{d}a_k^2+b_k^2=\sum_{k=1}^{d}Re(x_k)^2+Im(x_k)^2\\
&=\sum_{k=1}^{d}{\underbrace{|x_k|}_{\ge 0\forall k}}^2\\
&\ge 0
\end{align*}
Let $\braket{x,x}=0$:
\begin{equation*}
0=\sum_{k=1}^{d}{\underbrace{|x_k|}_{\ge 0\forall k}}^2 \quad \Rightarrow \quad x_k=0\forall k \quad \Rightarrow \quad x=0
\end{equation*}

Generally, all addends are greater zero, so they can impossibly cancel out and return zero. Thus, we must require them to be all trivially zero.

\end{enumerate}

\section*{Exercise 4}

\begin{enumerate}[(a)]

\item In order for the scalar product $\braket{x,Ay}$ to make sense, the linear map of matrix A has to be an endomorphism, A being a square matrix.\\

Let $z=Ay$:
\begin{equation*}
z_i=\sum_{j=1}^{n}a_{ij}y_j \quad \forall 1\le i\le n
\end{equation*}

The corresponding scalar product is:
\begin{align*}
\braket{x,Ay}=\braket{x,z}=\sum_{k=1}^{n}\overline{x_k}z_k=\sum_{k=1}^{n}\left(\sum_{j=1}^{n}a_{kj}y_j\right)=\sum_{k,j=1}^{n}\overline{x_k}a_{kj}y_j
\end{align*}

$A^\dagger=\left(\overline{A}\right)^T$ with $ \left(\overline{a_{ij}}\right)^T=a_{ji}$. Let $A^\dagger x=w$:
\begin{equation*}
w_i=\sum_{j=1}^{n}(\overline a_{ij})^T x_j \quad \forall 1\le i\le n
\end{equation*}

The corresponding scalar product is:
\begin{align*}
\braket{A^\dagger x,y}&=\sum_{k=1}^{n}\overline{w_k}y_k=\sum_{k=1}^{n}\overline{\left(\sum_{j=1}^{n}\overline{a_jk}x_j\right)}y_k=\sum_{k=1}^{n}\left(\sum_{j=1}^{n}\overline{\left(\overline{a_{jk}}x_j\right)}\right)y_k=\sum_{k=1}^{n}\left(\sum_{j=1}^{n}a_{jk}\overline{x-j}\right)y_k\\
&=\sum_{k,j=1}^{n}\overline{x_j}a_{jk}y_k
\end{align*}

A direct comparison reveals:
\begin{equation*}
\braket{x,Ay}=\sum_{k,j=1}^{n}\overline{x_k}a_{kj}y_j=\sum_{k,j=1}^{n}\overline{x_j}a_{jk}y_k=\braket{A^\dagger x,y}
\end{equation*}

\item 
\begin{align*}
& A^\dagger=\overline{(a_{ij})}^T=\overline{a_{ji}}\\
& B^\dagger=\overline{(b_{jk})}^T=\overline{b_{kj}}
\end{align*}

Let $AB=C$, such that:
\begin{align*}
& c_{ik}=\sum_{j=1}^{n}a_{ij}b_{jk}=\sum_{j=1}^{n}b_{jk}a_{ij}\\
\Rightarrow (& c_{ik})^\dagger=\overline{c_{ki}}=\sum_{k=1}^{n}\overline{b_{kj}}\overline{a_{ji}}=B^\dagger A^\dagger
\end{align*}

\item First of all, let us demonstrate that $Tr(AB)=Tr(BA)$.

\begin{equation*}
Tr(A)=\sum_{l=1}^{n}a_{ll} \quad \quad Tr(B)=\sum_{l=1}^{n}b_{ll}
\end{equation*}

Let $AB=C$, such that:
\begin{align*}
c_{ik}=\sum_{j=1}^{n}{a_{ij}b_{jk }}
\Rightarrow &\quad Tr(C)=Tr(AB)\\
&\quad \sum_{l=1}^{n}c_{ll}=\sum_{l=1}^{n}\sum_{j=1}^{n}\left({a_{ij}b_{jk}}\right)=\sum_{j,l=1}^{n}a_{lj}b_{jl}
\end{align*}
Let $D=BA$, such that:
\begin{align*}
d_{ik}=\sum_{j=1}^{n}{b_{ij}a_{jk }}
\Rightarrow &\quad Tr(D)=Tr(BA)\\
&\quad \sum_{l=1}^{n}c_{ll}=\sum_{l=1}^{n}\sum_{j=1}^{n}\left({b_{lj}a_{jl}}\right)=\sum_{j,l=1}^{n}b_{lj}a_{jl}
\end{align*}
Obviously:
\begin{equation*}
Tr(AB)=\sum_{j,l=1}^{n}a_{lj}b_{jl}=\sum_{j,l=1}^{n}b_{lj}a_{jl}=Tr(BA)
\end{equation*}

Consequently, we find that:
\begin{equation*}
Tr(S^{-1}AS)=Tr((S^{-1}A)S)=Tr(S(S^{-1}A))=Tr((S^{-1}S)A)=Tr(A)
\end{equation*}

Since the Matrix $S^{-1}AS$ is a base transformation matrix for the associated linear map of $A$, the trace must remain unchanged for base transformations.

\end{enumerate}

\end{document}